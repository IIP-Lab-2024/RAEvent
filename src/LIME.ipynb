{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from model.model.personalized_bert import EventBertModel\n",
    "from model.model.PairwiseLecardPLM1 import PairwisePLM1\n",
    "from formatter.PairwiseFormatter1 import PairwiseFormatter\n",
    "from config_parser import create_config\n",
    "from tools.init_tool1 import init_formatter\n",
    "from torch.autograd import Variable\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "# plm_path = \"/home/pub/fan/SCR/Pretrain_model/bert_base_chinese\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(plm_path)\n",
    "device = torch.device(\"cuda\")\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "def correct_state_dict(state_dict):\n",
    "    \"\"\"调整状态字典中的键名，以适应当前模型结构\"\"\"\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        # 移除所有不必要的'module.'前缀\n",
    "        new_key = k.replace(\"module.\", \"\")\n",
    "        new_state_dict[new_key] = v\n",
    "    return new_state_dict\n",
    "\n",
    "def load_model(model_path):\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model = PairwisePLM1()\n",
    "    # 使用修正函数处理状态字典\n",
    "    fixed_state_dict = correct_state_dict(checkpoint['model'])\n",
    "    model.load_state_dict(fixed_state_dict)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 假设你的模型保存路径为\n",
    "model_path = '/hhd2/fan/SCR/output/PairwiseLecardBertBase_re/3.pth'\n",
    "# model_path = '/hhd2/fan/SCR/output/PairwiseLecardSAILER_visual/3.pth'\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "\n",
    "PLM_vocab = \"/hhd2/fan/SCR/Pretrain_model/bert_base_chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(PLM_vocab, trust_remote_code=True)\n",
    "class ModelWrapper:\n",
    "    def __init__(self, model, tokenizer, max_len, cls_id, sep_id, query_len, cand_len, use_event=True):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.cls_id = cls_id\n",
    "        self.sep_id = sep_id\n",
    "        self.query_len = query_len\n",
    "        self.cand_len = cand_len\n",
    "        self.use_event = use_event\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _preprocess_lime_text(self, texts):\n",
    "        processed_data = []\n",
    "        for text in texts:\n",
    "            # 注意：这里的处理逻辑需要根据你的模型输入要求调整\n",
    "            # 假设tokenizer能处理原始文本并返回input_ids, segment_ids, mask, 以及event_ids（如果use_event为True）\n",
    "            tokenized = self.tokenizer.encode_plus(\n",
    "                text,  # 这里可能需要更复杂的逻辑来处理原始数据结构\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_token_type_ids=self.use_event,  # 根据是否使用事件信息决定是否返回token_type_ids\n",
    "            )\n",
    "            input_ids = tokenized['input_ids']\n",
    "            attention_mask = tokenized['attention_mask']\n",
    "            segment_ids = tokenized.get('token_type_ids', [0]*len(input_ids))  # 如果模型不需要则默认全0\n",
    "            \n",
    "            # 如果有事件信息的处理逻辑，这里需要添加相应的代码\n",
    "            if self.use_event:\n",
    "                # 假设事件ID处理逻辑在这里\n",
    "                event_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 90, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 14, 0, 40, 40, 0, 0, 0, 0, 0, 0, 0, 0, 67, 67, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 69, 69, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 64, 0, 0, 0, 67, 67, 0, 0, 0, 0, 67, 67, 0, 0, 0, 29, 0, 0, 0, 0, 0, 52, 52, 49, 49, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 67, 67, 0, 0, 0, 0, 0, 0, 44, 0, 0, 0, 28, 0, 0, 0, 0, 0, 0, 0, 0]  # 实现这部分逻辑以适应你的数据结构\n",
    "                \n",
    "            # 确保长度正确，可能需要根据你的实际逻辑调整\n",
    "            while len(input_ids) < self.max_len:\n",
    "                input_ids.append(0)\n",
    "                attention_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "                if self.use_event:\n",
    "                    event_ids.append(0)  # 假设添加的是事件ID的填充值\n",
    "            \n",
    "            processed_data.append({\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"segment_ids\": segment_ids,\n",
    "                \"event_ids\": event_ids if self.use_event else None,\n",
    "            })\n",
    "        \n",
    "        return processed_data\n",
    "\n",
    "    def predict(self, texts):\n",
    "        preprocessed_data = self._preprocess_lime_text(texts)\n",
    "        input_ids = torch.tensor([item[\"input_ids\"] for item in preprocessed_data], dtype=torch.long, device=self.device)\n",
    "        attention_mask = torch.tensor([item[\"attention_mask\"] for item in preprocessed_data], dtype=torch.long, device=self.device)\n",
    "        segment_ids = torch.tensor([item[\"segment_ids\"] for item in preprocessed_data], dtype=torch.long, device=self.device)\n",
    "        \n",
    "        if self.use_event:\n",
    "            event_ids = torch.tensor([item[\"event_ids\"] for item in preprocessed_data if item[\"event_ids\"] is not None], dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            event_ids = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _, _ = self.model(input_ids, attention_mask=attention_mask, token_type_ids=segment_ids, event_ids=event_ids)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        return probs.cpu().numpy()\n",
    "explainer = LimeTextExplainer(class_names=['Relevance', 'Irrelevance'])\n",
    "\n",
    "sample_text = \"2013年，被告人陈益桃受被告人邓某某指使，为解决邓与被害人杨某某之间的经济纠纷，纠集被告人瞿某某等人强行将被害人杨某某带至他处看管。期间，被告人陈益桃等人采用殴打及言语威胁等方式，逼迫杨某某签下悔过书并承诺放弃经济利益。被告人陈益桃等人又逼迫被害人杨某某出具欠条支付辛苦费5万元。\"\n",
    "\n",
    "query_len = 138\n",
    "cand_len =  0 \n",
    "max_len = query_len + cand_len + 2\n",
    "pad_id = tokenizer.pad_token_id\n",
    "sep_id = tokenizer.sep_token_id\n",
    "cls_id = tokenizer.cls_token_id\n",
    "wrapper = ModelWrapper(model, tokenizer, max_len, cls_id, sep_id, query_len, cand_len, use_event=True)\n",
    "exp = explainer.explain_instance(sample_text, wrapper.predict, num_features=6)\n",
    "\n",
    "# # 显示结果\n",
    "exp.show_in_notebook(text=sample_text)\n",
    "# tokens = ['[CLS]', '2013', '年', '，', '被', '告', '人', '陈', '益', '桃', '受', '被', '告', '人', '邓', '某', '某', '指', '使', '，', '为', '解', '决', '邓', '与', '被', '害', '人', '杨', '某', '某', '之', '间', '的', '经', '济', '纠', '纷', '，', '纠', '集', '被', '告', '人', '瞿', '某', '某', '等', '人', '强', '行', '将', '被', '害', '人', '杨', '某', '某', '带', '至', '他', '处', '看', '管', '。', '期', '间', '，', '被', '告', '人', '陈', '益', '桃', '等', '人', '采', '用', '殴', '打', '及', '言', '语', '威', '胁', '等', '方', '式', '，', '逼', '迫', '杨', '某', '某', '签', '下', '悔', '过', '书', '并', '承', '诺', '放', '弃', '经', '济', '利', '益', '。', '被', '告', '人', '陈', '益', '桃', '等', '人', '又', '逼', '迫', '被', '害', '人', '杨', '某', '某', '出', '具', '欠', '条', '支', '付', '辛', '苦', '费', '5', '万', '元', '。', '[SEP]']\n",
    "# print(len(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
